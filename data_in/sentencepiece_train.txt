최근 인공지능 기술이 기업 활동이나 실생활에 적용되는 사례들이 하나둘씩 늘어나고 있습니다. 예를 들어 항공, 금융, 의료 및 제약, 채용 및 HR, 음악, 출판 등이 대표적인 분야들입니다(참조링크). 이러한 응용분야의 확장은 기본적으로 그 엔진에 해당하는 인공지능 솔루션들의 성능향상에 기초하고 있습니다.
AI Network팀은 인공지능 솔루션들의 최신 트렌드를 이해하는데에 도움을 드리기 위해서 ‘최첨단 인공지능 솔루션들 (State-Of-The-Art AI Solutions)’이라는 시리즈 연재를 준비했습니다. 오늘은 그 첫 포스팅으로 최근 일부 성능평가에서 인간보다 더 높은 정확도를 보여서 화제를 모았던 구글의 인공지능 언어모델인 BERT (Bidirectional Encoder Representations from Transformers)에 대한 내용을 준비했습니다(참조링크).
BERT개발팀은 2018년 10월 11일 논문을 먼저 웹사이트에 공개하고 약 3주 후 약속대로 소스코드와 모델을 공개했습니다(참조링크). 이러한 BERT 결과물들의 공개는 흥미롭게도 주변의 NLP관련 개발자 또는 연구자들에게 ‘흥분’과 ‘고민’을 함께 던져준 듯 했습니다. 물론 흥분은 그 놀라운 성능에 대한 것인데요, 고민은 BERT개발팀의 접근방식이 기존의 연구개발 방식과 다소 다른 방향성을 가진다는 데에 기인합니다. BERT개발팀은 범용 모델 아키텍처와 범용 학습데이터를 사용하여 flexibility를 높임과 동시에 많은 머신리소스를 투자하는 방식으로 성능까지 높였습니다. BERT 공개 이후 인공지능 언어모델을 연구하는 사람들은 자연스럽게 연구 방향성에 대한 고민과 함께 연구 환경을 어떻게 구축할 것인가 하는 고민을 함께 해야할 듯 합니다.
본 포스팅에서는 먼저 BERT의 개발자들이 취하고 있는 접근방식과 아이디어들, 그리고 성능평가 결과들을 간략히 살펴보고 이런 내용들이 자연어 처리 커뮤니티와 인공지능 관련 개발자들에게 던지고 있는 메시지는 무엇인지, 그리고 AI Network와는 어떤 관련이 있는지를 살펴보고자 합니다.
BERT는 어떤 솔루션인가?
접근 방식
논문(참조링크)과 Reddit 포스팅(참조링크)에서 읽을 수 있는 BERT 개발자들의 접근방식은 기본적으로 (1) 범용 솔루션을 (2) 스케일러블 한 형태로 구현해서 (3) 많은 머신리소스로 훈련해서 성능을 높인다입니다.
논문에서 성능평가를 위해서 사용된 NLP 과제들은 모두 11개인데요, 각각의 과제에 특화된 모델을 만들어서 사용하지 않고 하나의 pre-trained 모델을 공통적으로 사용하고 있습니다. 대신 과제를 수행하기 전에 각각의 과제에 대한 약간의 fine-tuning을 적용하고 있습니다.
모델 아키텍처는 구글이 2017년에 발표한 범용 딥러닝 모듈 아키텍처인 Transformer를 사용하고 있습니다. Transformer의 가장 큰 장점은 병렬처리를 통한 학습 속도의 향상에 있다고 할 수 있습니다(참조링크). 논문에서 사용된 BERT 모델의 크기는 아래와 같습니다.
수억개(110M, 340M)의 parameter를 가지는 ANN은 실로 엄청난 크기입니다. 예를 들어 비교하자면 AlphaGo의 Policy Network(참조링크)은 약 4.6M개의 parameter로, image recognition 분야에서 유명한 ANN인 ResNet-50(참조링크)은 약 25M개의 parameter로 구성되어있다고 알려져 있습니다.
모델 아키텍처와 함께 학습 데이터도 범용 코퍼스를 사용하고 있습니다. 논문의 pre-training에 사용한 데이터는 BooksCorpus (800M 단어)와 English Wikipedia (2,500M 단어)라고 밝히고 있습니다. 만일 특정 과제에 대한 성능을 극대화할 목적으로 그 과제에 특화된 데이터를 사용한다면 논문에 제시된 결과보다 훨씬 더 좋은 성능을 보여주리라 기대할 수 있는 부분입니다.
사용한 머신리소스의 규모에 대해서는 소스 & 리소스 절에서 더 자세히 알아보도록 하겠습니다.
* 본 논문에서 소개된 내용들은 그림과 표를 포함하여 대부분 BERT 논문(참조링크)과 Reddit 포스팅(참조링크)을 참조한 것입니다. 더 자세한 내용은 원문을 참조하세요.
주요 특징들
Bidirectional Model
Fig 1(논문에서 발췌)은 BERT, OpenAI GPT, ELMo 각각에서 사용된 pre-training 모델 아키텍처를 도식화해서 보여주고 있습니다. OpenAI GPT에서는 left-to-right Transformer를, ELMo에서는 독립적인 left-to-right, right-to-left 모델을 결합해서 사용하는데 반해서 BERT에서는 하나의 bidirectional 모델을 사용해서 좌우 컨텍스트를 동시에 보는 구조를 가지고 있습니다.
Masked LM
Fig 2. Masked LM (논문에서 발췌).
앞에서 설명한 deep bidirectional 모델만을 사용하게 되면 모델 내부에 사이클이 만들어져서 training 자체가 무의미해질 수 있는 문제가 있습니다. 일반적으로 Neural Network에서 hidden layer의 수가 input parameter의 수 보다 많아질 경우 학습된 결과가 Identity Function 즉, 출력값이 입력값과 같아질 가능성이 높습니다. BERT 개발자들은 Denoising Autoencoder에서 입력값을 의도적으로 왜곡함으로써 이런 문제를 해결하는 데에(참조링크) 착안하여 Masked LM이라는 기법을 고안하였습니다(Fig 2 참조). 즉 입력 단어 배열에서 무작위적으로 단어를 선택해서 80%의 확률로 [MASK]로 치환하거나 10%의 확률로 임의의 단어로 치환하거나 나머지 10%의 확률로 바꾸지 않고 그대로 사용하게 됩니다.
Next Sentence Prediction
Fig 3. Next sentence prediction (논문에서 발췌).
성능평가에 사용된 과제들 중 많은 수가 문장들의 관계를 이해할 필요가 있는 것들이기 때문에 BERT 개발자들은 문장들의 관련성을 예측하는 모델도 학습해서 사용하고 있습니다. Fig 3에서 보는 것 처럼 monolingual corpus로부터 문장들을 추출하여 문장 쌍이 실제 연결된 문장들인 경우 IsNext로 labeling하고 무작위로 선택된 문장들인 경우 NotNext로 labeling해서 입력데이터로 사용하였습니다.
성능 평가
GLUE
GLUE (General Language Understanding Evaluation) 데이터셋은 다양한 자연어처리 과제들를 모아놓은 것으로 자연어 처리 솔루션들을 객관적으로 비교 평가하기 위한 목적으로 만들어졌습니다.
Table 1(논문에서 발췌)은 GLUE 데이터셋에 대한 평가결과를 보여주고 있습니다. 결과를 요약하자면 모든 과제에서 현존하는 가장 우수한 솔루션들보다 좋은 결과를 보여주었습니다. 이때 BERT-Base와 BERT-Large는 각각 평균 4.4%와 6.7%의 성능향상을 보이고 있습니다.
SQuAD
SQuAD (Standford Question Answering Dataset)는 크라우드소싱한 100k여개의 질문/답변 쌍으로 이루어져 있습니다. 과제는 문단과 질문이 주어졌을 때 문단 내에서 주어진 질문의 답에 해당하는 부분을 찾아내는 것입니다.
Table 2(논문에서 발췌)는 SQuAD에 대한 평가결과를 보여주고있습니다. 요약하면 가장 성능이 좋은 시스템들을 앙상블 및 싱글 환경에서 각각 앞서는 결과를 보여주었습니다. 이는 또한 Human의 결과보다 앞서는 결과입니다.
CoNLL-2003
CoNLL-2003 데이터셋은 200k개의 학습 단어들로 구성되어있는데 각각의 단어들은 Person, Organization, Location, Miscellaneous, or Other (non-named entity)로 annotate되어있습니다.
Table 3(논문에서 발췌)은 CoNLL-2003에 대한 평가결과를 보여주고 있습니다. BERT-Large가 다른 시스템들 보다 우수한 성능을 보여주고 있음을 확인할 수 있습니다.
SWAG
Situations With Adversarial Generations (SWAG) 데이터셋은 113k개의 ‘문장쌍 만들기 문제들’로 이루어져 있습니다. 과제는 주어진 첫 문장 뒤에 왔을 때 가장 자연스러운 문장을 주어진 4개의 선택지 중에서 고르는 것입니다.
Table 4(논문에서 발췌)는 SWAG 데이터셋에 대한 평가결과를 보여주고 있습니다. 결과는 BERT-Large가 ESIM+ELMo를 27.1%나 향상시킨 결과를 내고 있음을 볼 수 있습니다. 이는 Human (expert)의 결과보다 앞서는 결과입니다.
학습량과 성능
BERT 개발팀은 학습량(# of Steps)이 성능에 미치는 영향을 알아보기 위해서 Masked ML 및 Left-to-Right 아키텍처 기반 BERT-Base를 각각 다양한 학습량으로 pre-training한 결과로 성능을 평가해 보았습니다.
위의 그래프(논문에서 발췌)는 그 결과를 나타냅니다. 이 실험이 시사하는 바는 아래와 같이 요약됩니다.
질문: 그렇게 많은 학습량(128,000 words/batch * 1,000,000 steps)은 BERT의 성능에 필수적인가?
답변: 그렇다(수렴 그래프에서 확인 가능).
질문: Masked LM은 Left-to-Right 모델보다 수렴 속도가 더 느린가?
답변: 그렇다(그래프에서 확인 가능). 하지만 절대 성능에 있어서는 초반을 제외하고 항상 Masked LM이 Left-to-Right 모델을 앞서는 결과를 보여준다.
소스 & 리소스
BERT개발팀은 논문에 제시된 실험결과들을 얻기 위해서 다음과 같은 규모의 머신리소스를 사용했고 pre-training을 마치기 까지 4일 정도 소요되었다고 밝히고 있습니다.
BERT-Base: 4 Cloud TPUs (16 TPU chips total)
BERT-Large: 16 Cloud TPUs (64 TPU chips total)
이 리소스는 엄청난 양인데요, 개발자들은 만약 8개의 TESLA P100을 사용했다면 1년 넘게 걸렸을 수도 있다고 얘기하고 있습니다.
앞에서 언급했듯이 BERT 개발자들이 보여준 기본철학은
범용 솔루션을 설계하고
스케일러블 한 형태로 구현해서
많은 머신리소스로 훈련해서 성능을 높인다
입니다. 이런 접근방식의 성공은 NLP 연구/개발 분야에서의 한가지 중요한 변화를 의미합니다. 즉 NLP를 위한 AI 솔루션 개발은 이제 더이상 NLP 전문가들의 전유물이 아니라 기본적인 코딩 능력과 머신리소스의 운영 능력을 가진 모든 사람들에게 개방된다는 의미를 가집니다. 또한가지 중요한 점은 BERT-Base와 BERT-Large의 성능 차이에서 쉽게 확인되듯이 모델 사이즈(또는 리소스규모)와 성능 사이의 비례 공식은 결국 머신리소스에 대한 인공지능 솔루션의 의존도가 점점 커짐을 의미한다는 점입니다. 앞으로 막대한 양의 머신리소스를 효율적으로 확보/운영할 수 있느냐가 연구기관 또는 기업들의 경쟁력을 높이는 필수 조건이 되지 않을까라고 예상해 볼 수 있는 대목입니다.
앞서 언급했듯이 BERT개발팀이 이미 소스코드와 학습된 모델을 공개했기 때문에 (참조링크) 이제 원한다면 누구나 BERT를 사용할 수 있습니다. 하지만 누구나 새로운 모델을 학습시킬 수 있다는 의미는 아닙니다. 그만한 리소스가 필요합니다. AI Network에서 기여하고자 하는 바는 바로 이 부분입니다. AI Network이 궁극적으로 지향하는 바는 (1) 원한다면 누구나 cost-effective한 리소스를 제공받을 수 있고 (2) 자신이 가진 소스코드에 맞는 실행환경을 직접 구축하지 않더라도 쉽게 빌려 쓸 수 있게 하는 리소스 공유 서비스 플랫폼(aka Open Resource Platform)을 제공하는 것입니다. Open Resource에 대한 좀 더 자세한 소개는 아래 포스팅을 참고하세요.
Open Resource란 무엇이고 왜 필요한가? (BERT를 온라인에서 무료로 실행해보실 수도 있습니다)
오늘 포스팅은 여기서 마칩니다. 앞으로도 AI Network(ainetwork.ai)에 대한 많은 관심 부탁드립니다. 감사합니다.

토크나이징 (tokenizing) 은 문장을 토큰으로 나누는 과정입니다. 텍스트 데이터를 학습한 모델의 크기는 단어의 개수에 영향을 받습니다. 특히 Google neural machine translator (GNMT) 와 같은 Recurrent Neural Network (RNN) 기반 알고리즘들은 단어 개수에 비례하여 계산비용이 증가합니다. 그렇기 때문에 RNN 에 이용되는 vocabulary, word embedding 벡터의 종류가 제한됩니다. 하지만 vocabulary 의 개수가 제한되면 임베딩 벡터로 표현하지 못하는 단어가 생깁니다. 이 역시 미등록단어 문제입니다. RNN 기반 모델에서 이를 해결하기 위하여 Word Piece Model (WPM) 이 제안되었습니다. 단어를 한정적인 유닛 (finite subword units) 으로 표현합니다. WPM 은 언어에 상관없이 모두 적용할 수 있기 때문에 적용할 언어마다 해당 언어의 특징을 반영한 토크나이저를 만들지 않아도 됩니다. 그러나 모든 데이터 분석에 적합한 것은 아닙니다. WPM 이 유용할 수 있는 상황과 그렇지 않은 상황에 대해서 알아봅니다.

Word piece, units of words
Word Piece Model 은 제한적인 vocabulary units, 정확히는 단어를 표현할 수 있는 subwords units 으로 모든 단어를 표현합니다. 수백만개의 단어를 포함하는 데이터를 표현하기 위해서 bag of words model 는 단어 개수 만큼의 차원을 지닌 벡터 공간을 이용합니다. RNN 처럼 word embedding vectors 를 이용하는 모델은 단어 개수 만큼의 embedding vector 를 학습합니다. 단어의 개수가 많을수록 차원이 크거나 모델이 무거워집니다. 이를 해결하기 위해서는 제한된 (finite) 개수의 단어를 이용해야 합니다. 그러나 자주 이용되지 않는 수 많은 (long-tail) 단어들을 무시하면 미등록단어 문제가 발생합니다.

언어는 글자 (characters)를 subword units 으로 이용합니다. 영어는 알파벳을 유닛으로 이용합니다. 대부분의 영어 단어는 몇 개의 글자가 모여 하나의 단어를 구성합니다. 즉, 하나의 유닛이 어떤 개념을 지칭하기는 어렵습니다. 유닛이 모호성을 지닙니다. 중국어는 한자를 units 으로 이용합니다. 중국어도 여러 글자가 모여 하나의 단어를 이루기도 하지만, 한 글자로 구성된 단어도 많습니다. 영어보다는 유닛의 모호성이 줄어듭니다. 동음이의어의 문제가 남아있지만, 가장 모호성이 적은 방법은 모든 단어를 단어의 유닛으로 이용하는 것입니다.

그러나 토크나이징 방법에 따라 모호성이 적은 최소한의 유닛을 만들 수도 있습니다. 아래의 세 문장을 다음의 요소들로 나눈다면 이 유닛들은 의미를 보존하면서도 재활용이 될 수 있습니다.

Word Piece Model (sentencepiece) tokenizer
학습 데이터를 이용하지 않으면서도 위의 결과를 이끌 수 있는 heuristics 이 있습니다. ‘공연-‘, ‘개막-‘, ‘-냈어’, ‘-났어’은 유닛이기 때문에 유닛이 아닌 subwords 보다 자주 등장할 가능성이 높습니다. 만약 ‘공연-‘의 빈도수와 ‘개막공연-‘의 빈도수가 같다면 ‘공연-‘이나 ‘개막-‘은 유닛으로 이용하지 않아도 됩니다. 어자피 세 유닛 모두 ‘개막공연’을 나타내기 위한 부분들이니까요. 유닛이 자주 등장한다는 사실은 아마도 많은 언어의 공통적인 특징일 것입니다. 이를 이용한다면 language independent, universial tokenizer 를 만들 수도 있을 것 같습니다.

Word Piece Model (WPM) 은 이 개념을 이용하는 토크나이저입니다. 원 논문 (Sennrich et al., 2015) 에 적힌 예시입니다. Words 는 매우 다양합니다. makers, over 는 모두 자주 이용되기 때문에 그 자체를 units 으로 이용합니다. Jet 은 자주 등장하지 않는 단어이기 때문에 subword units 인 ‘J’ 와 ‘et’ 으로 나눕니다.

그 전에 모든 단어의 시작에는 underbar, ‘_’ 를 붙입니다. 그렇다면 ‘Jet’ 은 ‘_Jet’ 이 되어 ‘_J et’ 으로 나뉘어집니다. makers 는 ‘_makers’ 가 됩니다.

Word : Jet makers feud over seat width with big orders at stake
Wordpieces: _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake
Underbar 는 문장 생성, 혹은 subwords 부터의 문장 복원을 위한 특수기호 입니다. Underbar 없이 subwords 를 띄어두면, 본래 띄어쓰기와 구분이 되지 않기 때문입니다. 문장을 복원하는 코드는 간단합니다. 띄어쓰기 기준으로 나눠진 tokens 을 concatenation 한 뒤, _로 다시 나눠 tokenize 하거나 _ 를 빈 칸으로 치환하여 문장으로 복원합니다.

Byte-pair Encoding (BPE)
Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015) 에는 word pieces (subword units) 을 학습할 수 있는 간단한 코드가 적혀 있습니다.

아래의 vocab 은 low, lower, newest, widest 의 맨 뒤에 특수기호 ‘/w’ 를 넣은 뒤, 한글자 단위로 모두 띄어 초기화를 한 상태입니다. Character 는 기본 subword units 입니다. for loop 에서 빈도수가 가장 많은 bigram 을 찾습니다. 이 bigram 을 하나의 unit 으로 merge 합니다. 이 과정을 num_merges 만큼 반복합니다. vocab 의 value 는 빈도수 입니다. ‘low’ 가 5 번, ‘lower’ 가 2 번 등장했습니다.

위 코드를 실행시킬 때 print(best) 에 의하여 출력된 결과입니다. 처음 ‘es’ 는 9 번 등장하였기 때문에 (‘e’, ‘s’) 가 ‘es’ 로 합쳐집니다. ‘lo’ 는 7 번 등장했습니다만 Python dict 의 key order 순서에 의해 ‘es’ 가 우선적으로 merge 됩니다. 그리고 (‘es’, ‘t’) 가 병합되어 ‘est’가 됩니다. 같은 빈도수를 지닌다면 계속하여 병합이 됩니다. 하지만 ‘west’: 6 번, ‘dest’: 3 번 이기 때문에 ‘est’ 이후에는 (‘l’, ‘o’), (‘lo’, ‘w’) 가 병합됩니다.

Byte pair encoding 은 데이터 압축 방법입니다. 빈도수가 많은 최장길이의 substring 을 하나의 unit 으로 만들면 bit 를 아낄 수 있습니다.

토크나이저 입장에서는 많이 쓰이는 subwords 를 units 으로 이용하면, 자주 이용되는 단어는 그 자체가 unit 이 되며, 자주 등장하지 않는 단어 (rare words)가 subword units 으로 나뉘어집니다. 즉 WPM 은 각 언어의 지식이 없이도 빈번히 등장하는 substring 을 단어로 학습하고, 자주 등장하지 않는 단어들을 최대한 의미보존을 할 수 있는 최소한의 units 으로 표현합니다. 한국어에서 복합명사를 단일명사들로, 어절을 명사와 조사로 나누는 것과 비슷합니다.

다른 논문들을 살펴보면, 번역 엔진들에서는 자주 이용되는
k
1
 개의 단어는 따로 지정을 하고 그 외의 단어에 대해서
k
2
 개의 subword units 을 이용하여 토크나이징을 수행한다고 합니다.

논문의 원저자인 Sennrich 도 본인의 github 에 코드를 공개하였습니다. Python script 형식으로 input file 을 토크나이징하여 output file 로 만듭니다.

Codes (논문)
Sennrich et al (2015) 에 공개된 코드에 save, load 와 같은 기능을 추가하여 간단히 word piece model 을 구현하였습니다. 코드는 github에 공개하였습니다.

BytePairEncoder(n_units) 은 n_units 개수 만큼의 subword units 을 학습하는 클래스입니다. corpus 는 list of str (like) 입니다.

from bytepairencoder import BytePairEncoder

n_units = 5000
encoder = BytePairEncoder(n_units)
encoder.train(corpus)
학습이 끝낸 encoder의 tokenize() 를 이용하여 토크나이징을 할 수 있습니다.

tokens = encoder.tokenize(sent)
save 와 load 는 다음과 같습니다. model_path 를 입력합니다.

encoder.save(model_path)

loaded_encoder = BytePairEncoder()
loaded_encoder.load(model_path)
2016-10-20 의 뉴스를 이용하여 5,000 개의 subword units 을 학습한 뒤, 토크나이징을 하였습니다.

sent = '오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다'

encoder.tokenize(sent)
오패산터널의 경우, 잘 등장하지 않은 단어이기 때문에 모든 단어가 글자들로 나뉘어 집니다. ‘용의자’는 빈도수 상위 5000 등 안에 들지 못하였습니다. 그러나 ‘의자’는 5000 등 안에 들었기 때문에 하나의 subword unit 이 되었습니다. ‘연합뉴스’는 뉴스 문서에서는 매우 빈번한 단어이기 때문에 하나의 단어임을 확인할 수 있습니다.

오 패 산 터 널_ 총 격 전_ 용 의자 _ 검 거_ 서울_ 연합뉴스_ 경찰_ 관계 자들이_ 19일_ 오후_ 서울_ 강 북 구_ 오 패 산_ 터 널_ 인근 에서_ 사제 _ 총 기를_ 발사 해_ 경찰 을_ 살 해 한_ 용 의자 _ 성 모 씨를_ 검 거 하고_ 있다_ 성 씨는_ 검 거_ 당시_ 서 바이 벌_ 게임 에서_ 쓰 는_ 방탄 조 끼 에_ 헬 멧 까지_ 착 용한_ 상태 였다_
의미적으로 ‘용’ + ‘의자’ = ‘용의자’는 아닙니다. 하지만 composition 을 통하여 ‘용의자’의 의미를 파악하는 것은 더 이상 토크나이저의 몫은 아닙니다. 이 부분은 번역기, 문서 판별기 등의 알고리즘의 몫입니다.

Codes (Google release)
이 챕터는 2019-03-20 에 업데이트 되었습니다

Google 에서도 sentencepiece 라는 이름으로 코드를 공개하였습니다. Github 에는 C++ 로 구현된 코드가 구현되어 있으며, PyPI 는 이 코드를 파이썬에서 subprocess 를 띄어 스크립트로 이용하는 패키지 입니다.

Google 에서도 sentencepiece 라는 이름으로 Word Piece Model package 를 공개하였습니다. 설치는 pip install 로 가능합니다. 실습 환경에서의 버전은 sentencepiece==0.1.8 입니다.

pip install sentencepiece
학습 데이터는 빈 칸이 포함되지 않은 문서 집합입니다. 우리의 실습 데이터에는 정규화 과정에서 한문으로만 이뤄진 기사의 텍스트가 지워졌기 때문에 빈 줄이 포함되어 있습니다. 이 빈 줄을 제거하여 spm_input.txt 파일을 만듭니다.

import sentencepiece as spm

input_file = 'spm_input.txt'

corpus = ['list of string', 'should be not empty']
with open(input_file, 'w', encoding='utf-8') as f:
    for sent in corpus:
        f.write('{}\n'.format(sent))
Sentencepiece 는 C++ 로 구현되어 있으며, subprocess 를 이용하여 파이썬에서 스크립트를 실행시키는 방식입니다. Input 은 반드시 텍스트 파일이어야 합니다. command message 는 input, prefix, vocab_size 를 입력해야 합니다.

IPython notebook 에서는 파이썬 스크립트에서 오류가 날 경우 커널이 그대로 죽습니다. 파이썬 인터프리터에서 직접 작업하면 코드의 오류를 볼 수 있습니다. vocab_size 가 지나치게 작게 설정되면 모델이 학습을 하다 오류를 발생시킵니다. 당황하지 마시고 vocab_size 를 키우면 됩니다. 저는 2016-10-20 뉴스 기사를 이용하여 2000 개의 vocabulary (subwords) 를 학습하였습니다.

templates = '--input={} --model_prefix={} --vocab_size={}'

vocab_size = 2000
prefix = '2016-10-20-news'
cmd = templates.format(input_file, prefix, vocab_size)

spm.SentencePieceTrainer.Train(cmd)
학습이 끝나면 2016-10-20-news 라는 prefix 를 지니는 .model 파일과 .vocab 파일이 만들어집니다. 학습된 모델을 이용하려면 .model 을 로딩합니다.

sp = spm.SentencePieceProcessor()
sp.Load('{}.model'.format(prefix))
문장을 입력하면 subword list 로 출력할 수 있습니다.

sp.EncodeAsPieces('오늘의 연합뉴스 기사입니다')
# ['▁오늘', '의', '▁연합뉴스', '▁기사', '입니다']
혹은 subword idx list 로 출력할 수도 있습니다.

sp.EncodeAsPieces('오늘의 연합뉴스 기사입니다')
# [870, 6, 442, 786, 433]
.vocab 에서 학습된 subwords 를 확인할 수도 있습니다. 설정한대로 2000 개의 subwords 가 학습되었습니다.

with open('{}.vocab'.format(prefix), encoding='utf-8') as f:
    vocabs = [doc.strip() for doc in f]

print('num of vocabs = {}'.format(len(vocabs))) # 2000
일부를 실제로 학인해봅니다. Vocabulary 에는 unknown, 문장의 시작, 문장의 끝, 단어의 앞부분 등 네 가지의 special token 이 있으며, 그 외에는 을, 이, 의, 는 과 같은 조사들이 뒤따릅니다. _기자, _재배포 와 같은 도메인에서 자주 이용되는 단어들도 포함되어 있습니다. 앞서 의 가 6 번으로 encode 된 것을 보았습니다. 아래의 예시에서 <unk>, <s>, </s> 가 각각 0, 1, 2 번 subword 입니다. 의 는 6 번 subword 임도 확인할 수 있습니다.
Sentiment classification
네이버 영화 댓글과 평점을 이용하여 영화 평의 긍/부정을 구분하는 감성 분석을 수행하였습니다.

영화평의 감성 분석을 위해서는 데이터 전처리가 필요합니다. 영화 평점은 1 ~ 10 점을 지닙니다. 사람마다 점수의 기준이 다르기 때문에 4 ~ 7 점은 긍/부정을 명확히 판단하기 어려워 제외하였습니다. 또한 1 ~ 3 점을 서로 다른 클래스로 구분하는 것도 큰 의미가 없습니다. 그렇기 때문에 1 ~ 3 점은 negative, 8 ~ 10 점은 positive 로 레이블을 부여하였습니다.

성능의 이해를 위하여 KoNLPy 의 트위터 한국어 분석기와 성능을 비교하였습니다. Unigram 만 이용한 경우와 uni, bigram 을 함께 이용한 성능을 측정하였습니다. 문서 분류에서는 bigram 이 유용하다고 널리 알려져 있습니다. WPM 옆의 숫자는 units 의 개수입니다.
트위터 한국어 분석기는 unigram 만 이용하여도 91.9 % 의 성능을 보입니다. Word Piece Model 의 unigram 의 경우, unit 이 많을수록 성능이 올라갑니다. 20K 의 units 을 이용하면 오히려 트위터 한국어 분석기보다도 좋은 성능을 보입니다. 이는 빈번하게 등장하면서도 긍/부정을 판단하는데 유용한 subwords 가 존재하는데, 트위터 한국어 분석기는 이를 형태소들로 분해하였다는 의미입니다.

Word Piece Model 도 bigram 을 함께 이용하면 성능이 올라갑니다. 감성 분석은 bigram 이 중요한 features 입니다. ‘재미’라는 단어는 긍/부정을 판단하기 어렵습니다. 뒤에 따라오는 단어와 함께 ‘재미 + 없다’ / ‘재미 + 있다’ 처럼 bigram 이 이뤄져야 유의미한 features 가 됩니다. 트위터 한국어 분석기는 ‘재미없다 -> 재미 + 없다’로 나눕니다. 그렇기 때문에 bigram 을 이용하면 성능이 증가합니다. 91.91 %
→
 93.39 % 로 증가하였습니다.

그러나 Word Piece Model 을 이용하여도 성능의 상한선이 존재합니다. 10K 과 bigram 을 이용할 때 최고의 정확도인 93.47 % 을 보이며, 그 이후에는 bigram 을 이용하여도 성능이 내려갑니다. 과적합 (over-fitting) 의 문제라고 예상됩니다.

또한 50K 의 유닛을 이용할 때와 3K 의 유닛과 bigram 을 이용할 때의 성능이 92.65 % 와 92.67 % 로 비슷합니다. 실험을 수행할 때, 빈도수 20 이하의 uni, bigram 은 모두 제거를 하였습니다. 그 결과 3K 의 유닛과 uni + bigram 를 이용할 때의 차원은 56K 입니다. Bigram 으로 선택된 features 의 개수가 50K units 을 이용하는 WPM 의 unigram 과 비슷합니다. 즉, units 의 개수를 늘리면 bigram 으로 만들어져야 할 features 가 unigram 으로 만들어지는 효과를 얻습니다.
인공지능은 1959년에 MIT AI연구소를 설립한 매카시와 마빈 민스키, 카네기멜론 대학교에 인공지능 연구소를 만든 앨런 뉴웰과 허버트 사이먼과 같은 개척자들에 의해 1950년도에 실험 학문으로 시작되었다. 이들 모두는 1956년에 매카시, 민스키, IBM의 나단 로체스터 와 클라우드 샤논에 의해 조직되어 열린, 이미 언급된 다트머스 대학의 여름 AI 콘퍼런스에 참가하였다.

역사적으로, 인공지능 연구는 두 개의 부류 -- 깔끔이(Neats)와 지저분이(Scruffies) -- 로 나눌 수 있다. 깔끔이는 우리가 전통적 혹은 기호적(symbolic) 인공지능 연구라고 부르는 분야로서, 일반적으로 추상적인 개념에 대한 기호적 연산과 전문가 시스템(expert systems)에 사용된 방법론을 가르친다. 이와 상반된 영역을 우리는 지저분이(Scruffies) 또는 연결주의자(connectionist)라 부르는데, 시스템을 구축하여 지능을 구현/진화시키려고 시도하고, 특정 임무를 완수하기 위해 규칙적인 디자인을 하기보다는 자동화된 프로세스에 의해서 지능을 향상시키는 방식이다. 가장 대표적인 예로 신경망(neural network)이 있다. 이 두 가지 접근법은 인공지능 역사의 매우 초창기부터 함께 했다. 1960년대와 1970년대를 거치며 scruffy 접근법은 주목받지 못했지만, 1980년대 깔끔이 접근법의 한계가 명확해지면서 다시 주목 받게 되었다. 그러나 현재 두 가지 방식을 사용하는 그 어떤 최신의 인공지능 알고리즘도 확실한 한계가 있다는 것이 명확하다.

특히 1980년대에 들어서 Back propagation (인공지능 학습방법: Training Method)가 소개되면서 많은 연구가 진행되었음에도, 신경망을 이용한 인공지능은 아직 초보단계이다. 인공신경망 (Artificial Neural Networks)을 이용한 많은 연구가 현재에도 진행되고 있지만, 몇 가지 장애로 인해서 실용화하기엔 아직도 먼 기술이다. 인공신경망을 이용한 인공지능이 어느 정도 실용화되기 위해선 우선 실효성 있는 학습방법 (Training Methods)이 필요하다. Back propagation을 이용한 학습방법이 제안되어 연구되고 있지만, 완전한 학습을 이룰 수 없을 뿐만 아니라, 학습에 사용되는 data들이 서로 orthonormal해야 하는 조건 때문에 항상 불완전한 학습으로 끝나기 쉽다. (Converge to Local Mimimum, not to the optimal minimum: 지역최적해에 머뭄. 즉, 눈먼 장님이 가장 낮은 저지대를 찾는 경우 각 현재 지점에서 아래로 내려가려는 성질이 있는데 이때 눈먼 봉사이므로 특정 지점의 저지대에 도달한 경우, 그 지점에선 어디로 가거나 위로 올라가는 것만 있으므로 앞에 설명한 성질에 의해 바로 전에 찾은 저지대 남으려 하는 성질이 있다는 것을 의미함). 이러한 단점들을 보완하기 위해서 Fuzzy Logic, Neurofuzzy (Neural fuzzy logic) and Genetic Algorithms등을 이용한 학습방법이 연구되고 있으나 전망이 밝지만은 않은 상태이다.

미국의 DARPA(미 국방부 최신 기술 연구 프로젝트 관리국)과 일본의 5세대 컴퓨터 프로젝트에 의해서 1980년대 인공지능 연구는 엄청난 연구 기금을 지원 받을 수 있었다. 몇몇 인공지능 선각자들이 거둔 주목할 만한 결과에도 불구하고, 즉각적인 결과를 산출하는 데 실패하게 된다. 이것은 1980년대 후반 인공지능 연구 기금에 대한 대폭적인 삭감을 초래하였고, 인공지능 연구의 침체기를 뜻하는 인공지능의 겨울을 가져왔다. 1990년대, 많은 인공지능 연구가들은 좀 더 구체적인 목적아래 기계 학습, 로보틱스, 컴퓨터 비전과 같은 인공지능과 관련된 하위 영역으로 이동하였고, 순수한 인공지능에 대한 연구는 매우 제한적으로 수행되고 있다.

인공지능 이론의 발전
상당수 인공지능 연구의 (본래) 목적은 심리학에 대한 실험적인 접근이었고, 언어 지능(linguistic intelligence)이 무엇인지를 밝혀내는 것이 주목표였다(튜링 테스트가 대표적인 예이다).

언어 지능을 제외한 인공지능에 대한 시도들은 로보틱스와 집합적 지식(?)을 포함한다. 이들은 환경에 대한 처리, 의사 결정을 일치시키는 것에 중심을 두며 어떻게 지능적 행동이 구성되는 것인가를 찾을 때, 생물학과, 정치과학으로부터 이끌어 낸다. 사회적 계획성과 인지성의 능력은 떨어지지만 인간과 유사한 유인원을 포함한, 복잡한 인식방법을 가진 동물뿐만 아니라 특히 곤충들(로봇들로 모방하기 쉬운)까지 포함한 동물학으로부터 인공지능 과학은 시작된다. 여러가지 생명체들의 모든 논리구조를 가져온 다는 것은 이론적으로는 가능하지만 수치화, 기계화 한다는 것은 쉬운 일이 아니다.

인공지능 학자는 동물들은 인간들보다 모방하기 쉽다고 주장한다. 그러나 동물의 지능을 만족하는 계산 모델은 없다. 매컬러가 쓴 신경 행동에서 내재적 사고의 논리적 계산[3], 튜링의 기계와 지능의 계산[4] 그리고 리클라이더의 인간과 컴퓨터의 공생[5]가 기계 지능의 개념에 관한 독창적인 논문들이다.

존 루커스의 지성, 기계, 괴델[6] 과 같은 논리학과 철학기반의 기계지능의 가능성을 부인한 초기 논문들도 있다.[7]

인공지능 연구에 바탕을 둔 실질적인 작업이 결실을 거둠에 따라, 인공지능을 지지하는 사람들은 인공지능의 업적을 깎아내리기 위해 인공지능에 반대하는 사람들이 예전에는 '지능적'인 일이라고 주장하던 컴퓨터 체스나 음성인식 등과 같은 작업에 대해 말을 바꾸고 있다고 비난하였다. 그들은 이와 같이 연구 목표를 옮기는 작업은 '지능'을 '인간은 할 수 있지만, 기계는 할 수 없는 어떤 것'으로 정의하는 역할을 한다고 지적하였다.

(E.T. Jaynes에 따르면) 존 폰 노이만은 이미 이를 예측하였는데, 1948년에 기계가 생각하는 것은 불가능하다는 강의를 듣고 다음과 같이 말하였다. "당신은 기계가 할 수 없는 어떤 것이 있다고 주장한다. 만일 당신이 그 기계가 할 수 없는 것이 무엇인지를 정확하게 이야기해준다면, 나는 언제든지 그 일을 수행할 수 있는 기계를 만들 수 있다." 했다. 폰 노이만은 이미 그 전에 모든 처리절차(procedure)는 (범용)컴퓨터에 의해서 시뮬레이션 될 수 있다고 이야기함에 따라 쳐치-튜링 이론을 언급했다.

1969년에 매카시와 헤이스는 그들의 논문 "인공지능 관점에서 바라본 철학적인 문제들"에서 프레임 문제를 언급하였다.

인공지능의 탄생(1952-1956)
1940년대 후반과 1950년대 초반에 이르러서 수학, 철학, 공학, 경제등 다양한 영역의 과학자들에게서 인공적인 두뇌의 가능성이 논의되었다. 1956년에 이르러서, 인공지능이 학문 분야로 들어섰다.

인공두뇌학과 초기 신경 네트워크
생각하는 기계에 대한 초기 연구는 30년대 후기에서부터 50년대 초기의 유행한 아이디어에 영감을 얻은 것이었다. 당시 신경학의 최신 연구는 실제 뇌가 뉴런으로 이루어진 전기적인 네트워크라고 보았다. 위너가 인공두뇌학을 전기적 네트워크의 제어와 안정화로 묘사했으며, 섀넌의 정보 과학은 디지털 신호로 묘사했다. 또 튜링의 계산 이론은 어떤 형태의 계산도 디지털로 나타낼 수 있음을 보였다. 이런 여러 밀접한 연관에서, 인공두뇌의 전자적 구축에 대한 아이디어가 나온 것이다.[8] 월터의 거북이 로봇이 이 아이디어를 중요하게 포함한 연구의 예이다. 이 기계는 컴퓨터를 사용하지 않고 아날로그 회로를 이용했지만, 디지털의 전자적, 상징적 추리를 보여주기엔 충분했다.[9] 월터 피츠(Walter Pitts)와 워런 매컬러(Warren Sturgis McCulloch)는 인공 신경망에 기인한 네트워크를 분석하고 그들이 어떻게 간단한 논리적 기능을 하는지 보여주었다. 그들은 후에 신경 네트워크[10]라 부르는 기술을 첫번째로 연구한 사람이다. 피츠와 매컬러는 24살의 대학원생인 젊은 마빈 민스키를 만났고, 민스키는 1951년 첫번째 인경 네트워크 기계인 SNARC[11]를 구축했다. 민스키는 향후 50년동안 인공지능의 가장 중요한 지도적, 혁신적 인물 중 하나가 되었다.

튜링 테스트
1950년 앨런 튜링은 생각하는 기계의 구현 가능성에 대한 분석이 담긴, 인공지능 역사에서 혁혁한 논문을 발표했다.[12] 그는 "생각"을 정의하기 어려움에 주목해, 그 유명한 튜링테스트를 고안했다. 텔레프린터를 통한 대화에서 기계가 사람인지 기계인지 구별할 수 없을 정도로 대화를 잘 이끌어 간다면, 이것은 기계가 "생각"하고 있다고 말할 충분한 근거가 된다는 것이었다.[13]튜링 테스트는 인공 지능에 대한 최초의 심도 깊은 철학적 제안이다.

게임 인공지능
1951년에, 맨체스터 대학의 페란티 마크 1(Ferranti Mark 1) 기계를 사용하여 크리스토퍼 스트레이(Christopher Strachey)는 체커 프로그램을 작성했고, 디트리히 프린츠(Dietrich Prinz)는 체스 프로그램을 작성했다.[14] 아서 새뮤얼(Arthur Samuel)이 50년대 중반과 60년대 초반에 개발한 체커 프로그램은 결국 존경받는 아마추어에 도전할 수 있는 충분한 기술적 발전을 이룩했다.[15] 게임 인공지능은 역사 속에서 인공 지능의 발전의 척도로 계속 사용될 것이다.

상징 추론과 논리 이론
디지털 컴퓨터에 접할 수 있어진 50년대 중반에 이르러서, 몇몇 과학자들은 직관적으로 기계가 수를 다루듯 기호를 다루고, 사람처럼 기호의 본질적인 부분'까지 다룰 수 있을 것이라고 생각했다.[16] 이것은 생각하는 기계를 만드는 새로운 접근 방법이었다. 1995년에, 앨런 뉴얼(Allen Newell)과 허버트 사이먼(Herbert A. Simon)은 "논리 이론"을 구현했다. 그 프로그램은 결국 러셀과 화이트헤드의 '수학 원리'에 나오는 52개의 정리중 32개를 증명해냈고, 일부 새롭고 더 우아한 증거를 찾아내기도 했다.[17]

다트머스 컨퍼런스 1956년 : AI의 탄생
1956[18]년에 열린 다트머스 컨퍼런스는 마빈 민스키와 존 매카시, 그리고 IBM의 수석 과학자인 클로드 섀넌과 네이선 로체스터(Nathan Rochester)가 개최했다. 컨퍼런스는 "학습의 모든 면 또는 지능의 다른 모든 특성로 기계를 정밀하게 기술할 수 있고 이를 시물레이션 할 수 있다"[19]라는 주장을 포함하여 제안을 제기했다. 참가자는 레이 솔로모노프(Ray Solomonoff), 올리버 셀프리지(Oliver Selfridge), 트렌처드 모어(Trenchard More), 아서 새뮤얼(Arthur Smuel), 앨런 뉴얼(Allen Newell)과 허버트 사이먼(Herbert A. Simon)으로, 그들 모두 수십년동안 인공지능 연구에서 중요한 프로그램을 만들어온 사람들이었다.[20] 컨퍼런스에서 뉴얼과 사이먼은 "논리 이론"을 소개했고, 매카시는 Artificial Intelligence를 그들의 연구를 칭하는 이름으로 받아들이길 설득했다.[21] 1956년 다트머스 컨퍼런스는 AI 라는 이름, 목표점, 첫번째 성공과 이를 이룬 사람들, 그리고 넓은 의미의 AI의 탄생을 포함하는 순간이었다.[22]

황금기(1956~1974년)
다트머스 컨퍼런스 이후에, AI라는 새로운 영역은 발전의 땅을 질주하기 시작했다. 이 기간에 만들어진 프로그램은 많은 사람들을 "놀랍게(astonishing)[23]"만들었는데, 프로그램은 대수학 문제를 풀었고 기하학의 정리를 증명했으며 영어를 학습했다. 몇 사람들은 이와같은 기계의 "지능적" 행동을 보고 AI로 모든 것이 가능할 것이라 믿었다.[24] 연구가들은 개인의 의견 또는 출판물들을 통해 낙관론을 펼쳤고, 완전한 지능을 갖춘 기계가 20년 안에 탄생할 것이라고 예측했다.[25] ARPA(Advanced Research Projects Agency)같은 정부 기관은 이 새로운 분야에 돈을 쏟아부었다.[26]

작업들
많은 성공적인 프로그램과 새로운 발전 방향이 50년대 후반과 60년대에 나타났다. 이곳에는 AI 역사에 지대한 영향을 미친 것들을 기술했다.

탐색 추리
초기 AI 프로그램은 동일한 기본적인 알고리즘을 사용했다. 게임의 승리나 정리 증명 같은 어떤 목표 달성을 위해, 그들은 한발짝식 나아가는(step-by-step) 방식을 상용했다. 예를 들어 미로를 찾아갈때 계속 나아가면서 막힌 길이 있으면 다른 길이 있는 곳까지 되돌아 왔다가 다른 길로 가는 식이었다. 이런 패러다임은 "탐색 추리"라 불렸다.[27] 주요한 문제는, 간단한 미로에 있어서도 경로로 사용할 수 있는 수가 천문학적으로 많았다는 것이다. 연구가들은 추론 또는 경험적으로 찾은 규칙으로 정답이 아닌듯 보이는 경로를 지우는 방식을 사용했다.[28]뉴엘과 사미언은 "범용 문제 해결기(General Problem Solver)[29]"라 부르는 프로그램 속 알고리즘의 범용적인 버전을 포착하려고 노력했다. 다른 "검색" 프로그램은 기하학과 대수학의 문제를 해결하는 것처럼 인상적인 작업 - 허버트 게랜터(Herbert Gelenter)의 "기하학 해결기"나 민스키의 제자인 제임스 슬레이글(James Slage)의 SAINT - 을 수행하길 시도했다.[30] 다른 프로그램은 목표와 목표에 다가가기 위한 세부 계획을 검색했고, 여기에는 스탠포드에서 샤키(Shakey) 로봇의 동작을 제어하기 위해 개발한 STRIPS 시스템을 포함한다.[31]

자연어 처리
AI 연구의 중요한 목표는 영어와 같은 자연어로 컴퓨터와 의사소통할 수 있게하는 것이었다. 일찍이 다니엘 보로우(Daniel Bobrow)의 STUDENT라는 프로그램은 고등학교 수준의 대수학 단어 문제를 푸는 성공을 보였다.[32] '의미 망'은 개념을 다른 개념들 사이의 노드와 링크 관계로 나타낸다. 의미 망을 사용하는 첫번째 AI 프로그램은 로스 퀄리언(Ross Quillian)[33]이 작성했고 가장 성공이며 동시에 논쟁이 많았던 버전은 로거 섕크(Roger Schank)의 "개념 종속 이론(Conceptual dependency theory)"이다.[34] 조셉 웨이젠바움(Joseph Weizenbaum)의 ELIZA는 사람들이 그들이 대화를 나누는 때때로 상대가 컴퓨터가 아니라 사람이라고 생각할 정도의 질로 대화했다. 사실, ELIZA는 스스로 생각하여 말하지 않았다. 그 프로그램은 오직 판에 박힌 말을 하거나, 상대에게 방금 말한 말을 다시 해달라고 요청하거나, 상대가 한 말을 몇 개의 문법 법칙에 의해 파싱할 뿐이었다. ELIZA는 첫번째 채팅 프로그램이 되었다.[35]

마이크로월드
1960년대 후반에, MIT의 AI 연구소에 있던 마빈 민스키와 시모어 페퍼트는 마이크로월드 연구로 불리는, 인위적인 간단한 상황에 초점을 맞춘 AI 연구를 제안했다. 그들은 성공적인 과학자들이 자주 쉬운 이해를 위해 '마찰면'이라든지 '강체(물리학에서 결코 형태가 변하지 않는 물체)'같은 간단한 모델을 사용한다는 것에 집중했다. 이런 연구의 대부분이 평평한 평면 위의 다양한 형태와 색깔의 블록으로 이루어진 '블록 단위의 세계'에 초점을 맞추는 형식이었다.[36]

제라드 서스먼(Gerald Sussman)을 필두로 아돌프 구즈먼(Adolfo Guzman), 다비드 왈츠(David Waltz) 그리고 패트릭 윈스턴(Patrick Winston)이 마이크로월드 패러다임으로 기계 비전의 혁신을 이끌었다. 같은 시간에, 민스키와 페퍼는 블록을 쌓을 수 있는 로봇 팔을 제작했다. 마이크로월드의 영광스러운 성취는 테리 위노가드(Terry Winograd)의 SHRDLU이며, 이것은 보통의 일반 문장으로 소통해 작업을 계획하고 이를 실행할 수 있었다.[37]

낙관론
AI 연구의 첫번째 세대는 그들의 연구 결과에 대해 다음과 같이 예측했다.

1958년, 사이먼(H. A. Simon)과 뉴얼(Allen Newell) : "10년 내에 디지털 컴퓨터가 체스 세계 챔피언을 이길 것이다", 덧붙여 "10년 내에 디지털 컴퓨터는 중요한 새로운 수학적 정리를 발견하고 증명할 것이다"라고 말했다.[38]
1965년, 사이먼 : "20년 내에 기계가 사람이 할 수 있는 모든 일을 할 것이다."[39]
1967년, 마빈 민스키 : "이번 세기에 AI를 만드는 문제는 거의 해결 될 것이다."[40]
1970년, 마빈 민스키 : (Life 잡지를 통해서) "3~8년안에 우리는 평균정도의 인간 지능을 가지는 기계를 가지게될 것입니다."[41]
자금
1963년 6월, MIT는 220만 달러를 고등 연구 계획국(Advanced Research Projects Agency - 후에 DARPA로 알려짐)에게 제공받았다. 자금은 민스키와 맥카시가 5년전 설립한 "AI 그룹"이 포섭한 프로젝트 MAC에서 사용되었다. DARPA는 계속해서 매년 300만 달러를 70년대까지 제공했다.[42] DARPA는 또한 유사한 자금을 뉴얼과 사이먼의 CMU 프로그램과 스탠포드 AI 프로젝트에 제공했다.[43] 또다른 중요한 AI 연구소는 1965년 도널드 미치(Donald Michie)가 에든버러 대학교에 세웠다.[44] 이 4개의 시설은 계속해서 많은 연도에 걸쳐 학계의 주요한 AI연구소, 그리고 자금처로 존재할 것이다.[45] 자금은 몇가지 단서와 함께 제공됐다 : ARPA의 기획자 리클리더(J. C. R. Licklider)는 그의 조직은 "프로젝트가 아니라, 사람에게 투자"해야 한다고 믿었고, 연구자들이 어떤 방향이든 그들의 관심있는 쪽을 연구하도록 허용했다.[46] 이것은 MIT에 자유분방한 분위기를 생성했고 해킹 문화를 탄생[47]시키기도 했다. 그러나 이렇게 손을 떼고 지켜보는 형식의 지원은 얼마 지속되지 못했다.

AI의 첫번째 암흑기(1974-1980)
70년대에 이르자, AI는 비판의 대상이 되었고 재정적 위기가 닥쳤다. AI 연구가들은 그들의 눈앞에 있는 복잡한 문제를 해결하는데 실패했다. 연구가들의 엄청난 낙관론은 연구에 대한 기대를 매우 높여놓았고, 그들이 약속했던 결과를 보여주지 못하자, AI에 대한 자금 투자는 사라져버렸다[48]. 동시에, Connectionism 또는 뉴럴망은 지난 10년동안 마빈 민스키의 퍼셈트론(시각과 뇌의 기능을 모델화한 학습 기계)에 대한 파괴적인 비판에 의해 완전히 중지되었다.[49] 그러나 70년대 후반의 AI에 대한 좋지 않은 대중의 인식에도 불구하고, 논리 프로그래밍, 상징 추론과 많은 여러 영역에서의 새로운 아이디어가 나타났다.[50]

문제
1970년대 초, AI 프로그램의 가능성은 제한적이었다. 모든 문제에 걸쳐서 문제를 푸는 인상 깊은 작품들은 겨우 시험용 버전 정도였고, 어떤 의미에선 '장난감'에 가까웠다.[51] AI 연구는 70년대에 더 이상 극복할 수 없는 몇개의 근본적인 한계를 가지게 됐다.몇개의 한계를 통해 십여년 후에 극복되었고, 다른 몇 개는 오늘날까지 남아있다.[52]

컴퓨터 능력의 한계 : 정말 유용한 무언가를 이루기에는 메모리 또는 처리 속도가 충분하지 않았다. 예를 들어 로스 퀼리언(Ross Quillian)의 자연어 처리에서 성공적인 완수는 오직 20개의 단어 위에서 발휘되었는데, 이것은 메모리가 꽉 찼기 때문이었다.[53] ++한스 모라벡은 1976년에 컴퓨터가 지능을 가지기엔 여전히 수백만 배 약하다고 논증했다. 그는 비유를 들었는데, AI가 컴퓨터 능력을 필요로 하는 것은 항공기가 마력을 필요로 하는 것과 같다는 것이었다. 컴퓨터 영상에 대해서, 모라벡은 간단하게 계산하여 실시간으로 사람의 망막을 모션 캡처하려면 범용 컴퓨터가 초당 10^9 명령어(1000MIPS)를 처리해야 할 것이라고 추측했다[54]. 2011년경 실용적인 컴퓨터 영상 프로그램은 10,000~1,000,000 MIPS를 요구한다. 1976년경 5백만에서 8백만 달러사이에 판매되던 가장 빠른 슈퍼컴퓨터인 Cray-1은 오직 80~130 MIPS였고, 당시 전형적인 데스크탑 컴퓨터는 겨우 1 MIPS 남짓이었다.
폭발적인 조합 수와 비용이성 : 1972년에 리차트 카프(Hichard Karp)는 문제 해결에 지수적 시간이 요구되는 많은 문제를 보여주었다. 하찮은 문제일지라도 이런 문제의 최적의 해답을 찾는 데 상상할 수도 없는 컴퓨터의 시간이 요구되었다. 즉 지금까지 AI '장난감'에서 사용되었던 방법은 실제적으로 유용한 AI 시스템을 제작하는 데 용이하지 못했다.[55]
상징적 지식과 추론 : 영상 처리나 자연어 처리 같은 많은 중요한 AI 프로그램은 실제 세상에 대한 간단하지만 어마어마한 양의 정보를 필요로 한다. 그래야 프로그램이 자신이 보고 있는 것이 무엇인지, 또는 자신이 듣고 있는 것이 무엇인지 아이디어를 찾을 수 있기 때문이다. 이 요구는 아기들의 세상에 대해 알아나가는 것과 유사하다. 연구가들은 곧 요구되는 정보의 양이 엄청나게 광대하다는 것을 발견했다. 1970년대의 누구도 이런 데이터가 포함된 데이터베이스를 만들지 못했고, 누구도 이런 데이터를 프로그램 혼자 터득하는 방법을 알지 못했다.[56]
모라벡의 패러독스 : 이론을 제작하고 기하학적 문제를 해결하는 것은 컴퓨터에게 비교적 쉽지만, 얼굴을 인식하거나 장애물을 피해 방을 가로지르는 것은 엄청나게 어렵다. 이 설명은 왜 연구가들이 1970년대에 영상처리나 로봇에 대해 조금밖에 진전을 보이지 못했는지 아는 데 도움이 된다.[57]
프레임 문제, 자격 문제 : 존 맥캐시와 같은 연구가들은 규칙이 규칙 스스로의 구조를 변경하지 못하면 관련 계획 또는 기본 추론 일반 공제를 나타낼 수 없다는 것을 발견했다.[58]
자금 지원의 중단
영국 정보나 DARPA, NRC같은 AI 연구자들에게 자금을 주던 기관들은 연구 진행의 부진에 실망했고 결국 AI에 관한 방향성을 가진 자금 지원을 끊었다. 1966년 기계를 이용한 번역을 비판하는 보고서가 ALPAC에 제출되었을 때부터 이런 흐름이 시작되었다. 총 2천만 달러를 지원한 NRC도 지원을 멈췄다.[59] 1973년 라이트힐 보고서는 "장대한 목표(grandios objectives)"를 성취하는 데 실패한 영국의 AI 연구의 상태에 대해 비난했고 결국 영국의 AI 연구소는 해체되었다(보고서는 특히 AI 연구의 실패의 원인이 폭발적인 조합의 수라고 언급했다[60]).[61] DARPA는 CMU의 음성을 이해하는 연구의 연구자들에게 심하게 실망했고 연간 3백만 달러의 지원을 취소했다.[62] 1974년에 이르자 AI 연구에대한 투자는 찾기 어려워졌다. 한스 모라벡은 그의 동료의 비현실적인 예측에 의한 위기를 비난했다. "많은 연구가들이 많은 연구자는 과장을 증가시키는 웹에 휘말렸다."[63] 그러나 여기엔 다른 이슈가 있다 : 1969년 맨스필드의 수정안의 통과이후, DARPA는 자금 지원에 대해 "비직접적인 기초 연구보다, 임무 완수에 직결된 연구"를 수행하라는 증가하는 압력을 받고 있었다. 창조성 높은 지원, 자유분방한 연구는 1960년대와 함께 떠났고 DARPA에서 다시 오지 않을 것이다. 대신, 자금은 자동조정 탱크나 전투 관리 시스템과 같은 분명한 프로젝트와 명확한 목표를 향할 것이다.[64]

캠퍼스 전역의 비판들
몇 철학자들은 AI 연구가들에게 강력한 반대를 표했다. 초기 반대자들 중 괴델의 불완선성의 원리에 의해 컴퓨터 프로그램같은 시스템이 실제적으로 정확하게 사람과 같이 행할 수 없다고 주장한 사람은 존 루카스(John Lucas)이다.[65] 휴버트 드레이퓨즈(Hubert Dreyfus)는 60년대의 깨어진 약속을 조롱했고 AI의 가정을 비판했으며, 인간의 추론이 실제적으론 "상징적 진행"이 매우 적게 포함되어 있고 구현적, 본능적, 무의식적인 노하우[66][67]에 의해 처리된다고 주장했다. 존 시알리(John Searle)의 1980년대 제시된 중국인 방 문제는, 실제로 프로그램이 상징들을 '이해'할 수 없고 사용할 수 없음을 보여주려고 시도했다. 시알리는 만약 상징이 기계에게 아무 의미가 못된다면, 기계는 생각하는 것이 아니라고 주장했다.[68] 이 비난은 AI 연구가들에게 심각하게 작용하지 못했다. 비용이성과 상식적 지식에 관한 문제가 훨씬 더 즉각적이고 심각한 듯이 보였다. '노하우'와 '지향성'이 실제 프로그램을 만드는데 어떻게 다른지가 불분명했다. 민스키는 드레이퓨즈와 시알리를 향해 "그들은 오해했고, 무시될 것이다[69]"라고 했다. MIT에서 가르쳤던 드레이퓨즈는 냉대받았다 : 그는 나중에 AI 연구가들에게 "나와 점심 식사할 용기도 없다[70]"라고 평했다. ELIZA의 제작자 조셉 웨이즌바움(Joseph Weizenbaum)은 그의 동료인 드레이퓨즈가 전문적이지 않고 유치한 대우를 한다고 느꼈다. 웨이즌바움은 케네스 콜비(Kenneth Colby)가 쓴 DOCTOR와 임상치료 채팅봇에 대해서 심각하게 의심하기 시작했다. 웨이즌바움은 콜비가 그의 무심한 프로그램을 진지한 치료 도구로 여기는 걸 방해했다. 이 불화가 시작되고, 이 상황은 콜비가 웨이즌바움을 프로그램에 대한 공로로 인정하지 않았을때 도움이 되지 않았다. 1976년에 웨이즌바움은 컴퓨터 능력과 인간 추론(Computer Power and Human Reason)을 출판하며 인공 지능의 오용이 인간의 삶을 평가 절하시킬 수도 있다고 주장했다.[71]

퍼셉트론과 연결망의 어두운 시대
뉴럴 네트워크 형태의 퍼셉트론이 1958년 마빈 민스키의 고등학교 시절 친구였던 프랭크 로센블랫(Frank Rosenblatt)에 의해 도입되었다. 다른 AI 연구가들이 그러하듯, 그는 낙관론을 펼쳤고, "퍼셉트론은 결국 학습을 하고, 의사 결정을 하고, 언어 번역을 할 것이다"라고 예견했다. 60년대를 이끌던 패러다임 속의 연구 프로그램의 수행은 1969년 민스키와 페퍼의 책 퍼셉트론의 출판과 함께 갑자기 중지되었다. 이것은 퍼셈트론이 할 수 있는 일에 몇가지 심각한 제안이 있음을, 또 프랭크의 예견은 심하게 과장되어있음을 알렸다. 이 책의 파급력은 압도적이었다 : 향후 10년 동안 뉴럴 네트워크에 대한 거의 모든 연구가 중지되었다. 결국, 뉴럴 네트워크 영역을 회복할 연구원의 새로운 세대가 그 후에 인공지능의 중요하고 유용한 부분을 내놓았다. 로센블랫은 이 책을 보지 못했는데, 그는 문제의 책이 출판 되고 곧바로 보트 사고와 함께 사망했기 때문이다.[72]

깔끔이 : 논리, 프롤로그와 전문가 시스템
논리적 추론은 1958년 초에 AI 연구에서 존 맥카시가 제안하여 도입되었다.[73] 1963년 알렌 로빈슨(J. Alan Robinson)은 간단하게 추론을 컴퓨터에 구현시키는 분해와 통일 알고리즘을 발견했다. 그러나 맥카시와 그의 학생들이 60년대 후반에 했던 것과 같은 복잡하지 않은 구현은 본질적으로 다루기 힘들었는데, 간단한 정리를 증명하기 위해 천문학적 단계가 필요했다.[74] 더 성공적인 결실을 맺는 논리적 접근은 70년대 에딘벌 대학의 로버트 코왈스키(Robert Kowalski)가 개발했고 곧 프랑스의 연구가인 알라인 콜메루엘(Alain Colmerauer)과 성공적인 논리 프로그래밍 언어인 프롤로그를 만든 필립 오우셀(Philippe Roussel)과의 협업을 이끌어냈다.[75] 프롤로그는 다루기 쉬운 계산을 허용하는 논리의 부분을 사용한다. 규칙은 계속적으로 영향을 미쳤고, 에드워트 페이젠바움(Edward Feigenbaum)이 기대하던 시스템 기초를 제공했으며 알렌 뉴엘과 허버트가 계속 연구하도록 만들었다. 사이먼은 Soar과 인식에서의 통일 이론을 이끌었다.[76] 논리적으로의 접근을 비판하는 지적은, 드레이퓨즈가 했던데로, 사람이 문제를 해결할때 논리를 거의 사용하지 않는다는 것이었다. 피터 왓슨(Peter Waon), 엘리아노 로츠(Eleanor Rosch), 아모스 스벌스키(Amos Tversky), 다니엘 카니만(Daniel Kahneman)을 비롯한 심리학자들이 이를 증명했다.[77] 맥칸시는 이에 대해서 이 증명이 무관하다고 답했다. 그는 정말 필요한 기계란 사람처럼 생각하는 것이 아니라 문제를 해결할 줄 아는 기계라고 일축했다.[78]

지저분이 : 프레임과 스크립트
맥카시의 접근에 대한 비평가들의 대다수가 그의 동료인 MIT 소속이었다. 마빈 민스키와 사무엘 페퍼와 로저 샹크는 기계를 사람처럼 느껴지도록 만드는 "이야기 이해"와 "물체 인식"의 문제를 해결하려고 노력했다. "의자"나 "음식점" 같은 일반적인 개념을 사용할때 사람들은 모두 비논리적으로, 사람들이 통용하는 범용적 가정을 함께 했다. 불행하게도 이런 부정확한 가정들은 논리적 절차로 대표하기가 힘들었다. 제라드 서스먼(Gerald Sussman)은 "본질적으로 부정확한 개념을 설명하기위 해 정확한 언어를 사용하는 순간 그들은 더이상 부정확하다고 말할 수 없다"[79]라고 표했다. 또한 섕크는 이에 대해 "비논리적" 접근 즉 "지저분이"가 맥카시, 코와스키, 페이젠바움의 "깔끔이" 패러다임과 반대에 있다고 평했다.[80]

1975년 세미나 보고서에서, 민스키는 "지저분한" 많은 그의 동료 연구자들이 무언가에 대한 우리의 모든 상식적 가정을 포착하는 프레임워크를 도구로 사용했다고 적었다. 예를 들어 우리가 새라는 개념을 생각할때, 즉시 '난다', '벌레를 먹는다'와 같은 다양한 사실들 또한 떠올린다. 떠올린 것들이 항상 사실은 아니고 또 "논리적"으로 이것들이 공제가 되지는 않는다. 그러나 이런 가정들의 구조는 우리가 말하고 생각하는 문장의 부분을 차지한다. 그는 이 구조를 "프레임"이라 칭했다. 섕크는 프레임의 설명에 대해서 영어로된 짧은 스토리에 대한 답변을 성공적으로 하기 위한 "스크립트"라 불렀다.[81] 수년 후 객체지향 프로그래밍에서 AI 연구에서 쓰였던 프레임에서 나온 '상속'이라는 개념을 채택할 것이다.

Boom 1980-1987
1980년대에는 전 세계적으로 사용된 ‘전문가 시스템’이라고 일컫는 인공지능 프로그램의 형태였고 인공지능 검색에 초점이 맞춰졌다. 같은 시기에 일본 정부는 자신들의 5세대 컴퓨터 프로젝트와 인공지능에 적극적으로 투자하였다. 1980년대에 존 홉필드와 데이비드 루멜하트의 신경망 이론의 복원이라는 또 다른 사건이 있었다.

전문가 시스템의 상승
전문가 시스템은 특정 지식의 범위에 대해 문제를 해결해주거나 질문에 대답해주는 프로그램이며 전문가의 지식에서 파생 된 논리적 법칙을 사용하였다. 최초의 실험은 1965년 Edward Feigenbaum과 그의 제자 Dendral이 시작하였고 분광계로부터 화합물을 식별하는 실험이었다. MYCIN은 1972년에 개발되었고 전염되는 혈액 질환을 진단하였다. 이러한 접근법(실험)은 타당성이 입증되었다.[82]

전문가 시스템은 소규모의 지식 영역에 대해서는 스스로 제한을 둠으로써 상식 문제를 피하였다. 그리고 그들의 단순한 디자인은 프로그램을 만드는 것을 상대적으로 쉽게 하였다. 모든 프로그램은 유용성이 입증되어야 하지만 AI는 이 점을 달성할 수 없었다.[83]

1980년, XCON이라 불리는 전문가 시스템은 디지털 장비 회사인 CMU에서 완성되었다. 이 시스템은 매년 4천만 달러를 절약시켜주며 매우 큰 성과를 나타냈다.[84] 전 세계의 회사들은 1985년에 1억 달러 이상을 AI에 사용하여 이를 개발하고 전문가 시스템을 배포하였다. Symbolics, Lisp Machines과 같은 하드웨어 회사와 IntelliCorp, Aion 등의 소프트웨어 회사들이 이를 지원하면서 같이 성장하였다.[85]

지식 혁명
전문가 지식들을 포함하면서 전문가 시스템의 힘은 두각을 나타내었다. 이것은 1970년대 내내 연구하였던 AI 연구 기법의 새로운 방향 중 일부분이었다. “AI 과학자들은 지능이란 것이 다른 방법들로 많은 양의 다양한 지식들을 사용하는 능력에 기반한 것이라고 의심하기 시작했다.”[86] 지식 기반 시스템과 지식 엔지니어링은 1980년대 AI 연구자들의 메인 포커스가 되었다.[87]

또한 1980년대에는 일반인들이 모두 알 만한 일상적인 사실들을 모두 포함한 아주 거대한 데이터베이스를 만들어 상식 문제에 대한 직접적 해결을 시도한 Cyc의 탄생을 볼 수 있었다. 이 프로젝트를 이끈 Douglas Lenat는 지름길은 없다고 말했다. - 기계가 인간의 개념을 알게 하기 위한 단 한 가지 길은 그들을 가르치는 것이다. 이 프로젝트는 수 십 년 동안 완료될 것이라 생각되지 않았다.[88]

돈은 되돌아온다 : 5세대 프로젝트
1981년, 일본의 국제 무역과 산업 부서는 5세대 컴퓨터 프로젝트를 위해 8억 5천만 달러를 확보해 두었다. 그들의 목적은 기계가 사람처럼 프로그램을 작성하고 대화를 수행할 수 있는 시스템과 언어를 번역하거나 그림을 해석하는 것이었다. 그들은 프로젝트를 위해 기본 컴퓨터 언어로 Prolog를 선택하였다.[89]

다른 나라들은 그들만의 고유한 프로그램을 개발하였다. UK는 3억 5천만 달러를 들여 Alvey 프로젝트를 시작했다. 미국 회사들의 컨소시엄은 정보기술과 AI안의 거대한 프로젝트를 투자받기 위해 마이크로 전자공학과 컴퓨터 기술 협력이라는 형태를 취했다.[90][91] 또한 1984에서 1988년 사이에 DARPA는 전략적 컴퓨팅 계획을 설립하고 AI에 대한 투자를 세배로 늘렸다.[92]

신경망 이론의 복귀
1982년 , 물리학자 John Hopfield는 (현재 ‘Hopfield net’이라고 불리는) 완벽한 새로운 길에서 정보를 프로세스하고 배울 수 있는 신경망의 형태를 증명해냈다. 이 시기에, David Rumelhart는 (Paul Werbos에 의해 발견된) “역전파”라고 불리는 신경망을 개선하기 위한 새로운 방법을 알리고 있었다. 이러한 두 가지 발견은 1970년 이후 버려진 신경망 이론이라는 분야를 복구시켰다.[93][94] 새로운 분야는 1986년 분산 병렬처리의 형태로부터 영감을 받았고 이와 같은 형태로 통일되었다. 2권 분량의 논문 집합은 Rumelhart와 물리학자인 James McClelland에 의해 편집되었다. 신경망은 1990년대에 광학 문자 인식 및 음성 인식과 같은 프로그램의 구동 엔진으로 사용되며 상업적으로 성공했다.[95][96]

AI의 두번째 암흑기 1987-1993
AI와 비즈니스 커뮤니티의 매력은 상실했고 경제 거품이라는 고전적 형태의 1980년대에 빠졌다. 붕괴는 정부기관과 투자자들의 ‘해당 분야는 계속해서 비판에도 불구하고 진보해왔다.’는 인식에 비롯된 것이었다. 로봇 공학 분야에 관련 된 연구원인 Rodney Brooks 와 Hans Moravec는 인공지능에 대한 완전히 새로운 접근 방식을 주장하였다.

인공지능의 겨울
1974년에 전문가 시스템에 대한 열정이 통제할 수 없을 정도로 퍼져나가고 이에 대한 실망이 확실히 따라올 것이라는 걱정이 있었고 이 때 투자가 끊기고 살아남은 연구원들에 의해서 “AI winter”이라는 단어가 만들어졌다.[97] 그들의 두려움은 AI에 대해 일련의 재정적 차질이 있었던 1980년 말에서 1990년대 초반에 잘 나타난다. 이 AI winter 기간의 첫 번째 사건은 1987년에 특성화된 AI 하드웨어 시장이 갑자기 무너진 것이다. 1987년에 애플이나 IBM의 데스크탑 컴퓨터들은 급격히 빨라지고 성능이 좋아졌다. 또한 Symblics과 기타 회사들이 만든 데스크탑 컴퓨터 보다 더 비싼 Lisp 기기들보다도 더욱 좋은 성능을 나타냈다. 즉, 더 이상 Lisp 기기들을 살 이유가 사라진 것이다. 전체산업 1억 달러의 절반의 가치가 하룻밤에 사라졌다.[98] 결국 최초의 성공한 전문가 시스템인 XCON은 유지하기에 너무 비싸다는 것이 증명되었다. 업데이트하기에도 너무 어려웠고 학습도 되지 않았다. 이 전문가 시스템은 또한 일반적이지 않은 질문을 했을 때 괴상한 행동을 하는 일명 "brittle" 이었고 그들은 일찍이 발견된 이러한 문제들에 의해 결국 희생되었다. 전문가 시스템은 특별한 경우에서만 유용할 뿐이었다.[99] 1980년대 후반, Strategic Computing initiative는 AI의 투자를 자르는 데 공이 컸다. DARPA의 새로운 리더쉽은 AI는 이 다음의 파도가 아니라고 결정했고 즉각적인 결과를 나타낼 수 있는 것으로 보이는 프로젝트에 직접적인 투자를 하는 방향으로 결정했다.[100] 1991년에는 1981년에 일본에서 5세대 프로젝트의 목표 리스트에 적은 것만큼 성과가 나오지 않았다. 실제로 대화를 계속 이어나가는 것과 같은 어떤 것들은 2010년까지 달성되지 않았다. 다른 인공 지능 프로젝트와 마찬가지로, 실제 가능했던 것보다 기대가 훨씬 컸다.[101]

몸통을 갖는 것의 중요성: Nouvellle AI and embodied reason
1980년대 후반 , 몇몇 연구원들이 로봇 공학을 기반으로 인공 지능에 완전히 새로운 접근법에 대해 찬성하였다.[102] 그들은 실제 지능을 보여주려면 기계에도 몸통이 필요하다고 믿었다. - 기계 또한 이 세상에서 인식하고, 이동하고, 살아남고 거래할 줄 알 필요가 있다. 그들은 이런 감각 운동 기술은 상식적인 추론과 같은 더 높은 단계의 기술이 필요하다고 말했고 실제로 추상적인 추론은 인간의 가장 흥미롭거나 중요한 기술이다. 그들은 지능을 바닥에서부터 지어야 한다고 내세웠다.[103] 인공 두뇌와 제어 이론에서부터 얻은 접근법은 1960년대까지 인기가 없었다. 또 다른 선구자인 David Marr는 신경 과학 이론으로 한 그룹의 비전을 이끌어 성공적인 배경으로 1970년대에 MIT에 들어왔다. 그는 모든 상식적인 접근법(McCarthy's logic and Minsky's frames)을 거절했고 AI는 시각에 대한 육체적인 기계장치를 심볼릭 프로세싱 하기 전에 가장 바닥에서부터 위로 이해할 필요가 있다고 말했다.[104]

1990년에 Elephants Don't Play Chess 논문에서, 로봇 공학 연구자인 Rodney Brooks 는 직접적으로 물리적 심볼 시스템 가설에 초점을 맞추었고 심볼들은 항상 필요한 것은 아니라고 말했다. “세계는 그 자체만으로 가장 훌륭한 모델이다. 이것은 항상 최신이며 모든 세부사항이 존재한다. 비결은 적절히 그리고 충분히 자주 감지하는 것이다.[105] 80년대와 90년대에 많은 cognitive 과학자들은 또한 사고방식의 심볼 처리 모델을 거절하고 추론에 몸통은 필수적이라고 말했고 이러한 이론을 embodied mind 이론이라고 불렀다.[106]

AI 2018-현재
지금보다 반세기는 더 오래된 AI의 분야는 마침내 가장 오래된 목표 중 몇 가지를 달성했다. 이것은 비록 뒷받침해주는 역할이었지만 기술 산업에 걸쳐 성공적으로 사용되었다. 몇 가지 성공은 컴퓨터의 성능이 증가했기 때문이고 또 다른 몇 가지는 고립된 문제들에 대해 집중하였고 높은 과학적 의무감으로 해 나갔기 때문에 해결되었다. 적어도 비즈니스 분야에서의 AI의 평판은 여전히 처음 같지 않다. 이 분야 내에서는 1960년대 세계의 상상이던 인간 수준의 지능의 꿈을 실현하는 것이 실패로 돌아갔다는 이유로 몇 가지 합의를 하였다. 하위 파트에서 AI의 일부분을 도와주던 모든 요소들은 특정 문제나 접근 방식에 초점이 맞추어졌다.[107]그 후, AI는 여태 해왔던 것보다 더욱 신중해졌고 더욱 성공적이였다.

Milestones and Moores' Law
1997년 5월 11일, 디프 블루는 당시 세계 체스 챔피언이던 게리 카스파로프를 이긴 최초의 체스 플레이 컴퓨터가 되었다.[108] 2005년 스탠포드의 로봇은 DARPA 그랜드 챌린지에서 연습해 보지 않은 사막 도로 131마일을 자동으로 운전하여 우승하였다.[109] 2년 뒤, CMU의 한 팀은 DARPA 도시 챌린지에서 모든 교통 법규를 지키고 교통 혼잡 속에서 자동으로 55 마일을 길을 찾았다.[110] 2011년 2월, Jeopardy! 퀴즈 쇼의 시범 경기에서 IBM의 대답하는 시스템 왓슨은 상당히 여유롭게 Brad Rutter 과 Ken Jennings 두 명의 뛰어난 Jeopardy! 챔피언들을 이겼다.[111]

이러한 성공은 혁신적인 새로운 패러다임 때문이 아니라 번거로운 엔지니어 스킬과 매우 뛰어난 성능을 가진 오늘날의 컴퓨터에서 비롯된 것이다.[112] 실제로, Deep Blue의 컴퓨터는 1951년 Christopher가 체스 하는 법을 가르친 마크 1보다 1천만 배 빨랐다.[113] 이 엄청난 증가는 무어의 법칙에 의해 측정되는데 이것은 2년마다 컴퓨터의 메모리 속도와 양은 두 배씩 늘어난다는 이론이다. 최초 컴퓨터 성능의 근본적인 문제는 느리지만 서서히 극복되고 있었다.

지능형 에이전트
1990년대 동안에는 ‘지능형 에이전트’라고 불리는 새로운 패러다임이 다 방면에서 수용되고 있었다.[114] 비록 이전의 연구자들은 'divide and conquer' 모듈러를 제안하고 AI에 접근하였지만[115] 지능형 에이전트는 Judea Pearl, Allen Newell 등 다른 이들이 AI를 연구하는데 있어서 결정론과 경제성이라는 개념을 가져오기 전까지 현대식 형태를 갖추지 못했다.[116] 경제학자들의 합리적 에이전트라는 정의와 컴퓨터 과학자들의 객체 혹은 모듈러 정의가 합쳐졌을 때 지능형 에이전트의 패러다임이 완성되었다.

지능형 에이전트 시스템은 환경을 인식하고 성공을 가장 극대화할 수 있는 행동을 취한다. 이러한 정의에 의하면 인간과 인간의 조직처럼, 예를 들어 회사처럼 특정 문제를 해결하는 간단한 프로그램을 지능형 에이전트라고 한다. 지능형 에이전트는 AI 연구자를 “the study of intelligent agents"로 정의한다. 이것은 AI의 정의의 일부를 일반화한 것이다. 이것은 인간의 지능을 넘어 모든 종류의 지능의 연구를 추구한다.[117]

이러한 패러다임은 당시 연구자에게 고립 문제에 대해 연구하고 다양하고 유용한 해결법을 찾는 것을 허가하였다. 또한 서로서로 문제와 해결책을 공통의 언어로 표현하였고 추상적 에이전트를 사용한 경제학이나 제어 이론 등과 같은 다른 개념에도 사용되었다. 어느 날 연구자는 지능형 에이전트의 상호 작용에서 더 다양하고 지능적인 시스템을 만들기로 하였고 완전한 에이전트 아키텍처가 되기를 바랐다.[118]This is how the most widely accepted textbooks of the 21st century define artificial intelligence. See Russell & Norvig 2003, p. 32 and Poole, Mackworth & Goebel 1998, p. 1[119]

Victory of the neats
AI 연구자는 과거에 사용했던 것보다 더욱 정교한 수학적 도구를 사용하여 개발하기 시작했다.[120] 해결하는 데 AI가 필요한 수많은 문제들이 존재하고 있다는 인식은 수학, 경제학 또는 오퍼레이션 연구 등의 분야에서 이미 연구자들이 AI를 사용하여 실현하고 있었다. 공유된 수학적 언어는 높은 수준의 협력, 좋은 평판, 여러 분야를 성공적으로 이끌고 측정과 증명이 된 결과들의 성취를 가능하게 하였다. AI는 더 엄격한 과학 학문이 되었다. Russell & Norvig (2003) - this as nothing less than a "revolution" and "the victory of the neats".[121][122] Judea Pearl의 매우 영향이 큰 1988년 책은 AI에 결정론과 확률을 대입시켰다.[123] 사용중인 많은 새로운 도구 중 Bayesian networks, hidden Markov models, information theory, stochastic modeling 그리고 기존의 고전적이 방법들이 최적화되었다. 보다 정밀한 수학적 설명들은 신경망 네트워크와 진화 알고리즘과 같은 연산 지능적 패러다임을 위해 개발되었다.[124]

AI behind the scenes
AI 연구자들에 의해 최초로 개발된 알고리즘은 거대한 시스템의 일부로 나타나기 시작했다. AI는 매우 어려운 문제[125]들을 해결했고 데이터 마이닝, 산업 로봇공학, 논리학[126], 음성 인식[127], [151]은행 소프트웨어,[128]의학적 진단, 구글 검색 엔진[129] 등 여러 기술들은 기술 산업[130]에 매우 유용하다는 것이 증명되었다.

AI 분야는 이러한 성공에 대해 매우 낮은 신뢰를 받았다. AI의 훌륭한 혁신 중 대부분은 컴퓨터 과학의 도구에서 또 다른 기능으로 세분화되었다.[131]Nick Bostrom은 "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."라고 말했다.[132] 1990년대 AI 분야의 많은 연구자들이 고의로 자신의 프로젝트를 다른 이름으로 불렀다. 일부 이러한 현상은 그들의 분야가 AI와 근본적으로 다르다고 여겼기 때문이거나 또는 새로운 이름이 투자받기 쉬웠기 때문일 것이라고 한다. 적어도 상업 분야에서는 연구자에 대해 AI winter에 있었던 실패했던 계약이 꼬리표처럼 따라다녔고 2005년에 뉴욕 타임즈에서는 “컴퓨터과학과 소프트웨어 엔지니어들은 광기에 싸인 몽상가처럼 보일 두려움 때문에 인공 지능이란 용어를 피했다.” 라고 소개되었다.[133][134][135]

HAL 9000은 어디에 있는가?
1968년 Arthur C. Clarke와 Stanely Kubrick은 2001년에는 기계가 인간과 유사하거나 또는 인간의 용량을 뛰어넘는 지능을 가진 존재가 되었을 것이라고 상상했다. 그들이 만든 HAL 9000이라는 캐릭터는 2001년에 이러한 기계가 존재할 거라고 믿는 많은 AI 연구자들의 공유된 믿음을 기반으로 만들어졌다.[136] Marvin Minsky는 “그래서 왜 우린 2001년에 HAL을 얻지 못했나?”라는 질문을 하였다.[137] 대부분의 연구자들이 신경망이나 유전자 알고리즘의 상업적 용도의 프로그램을 추구 했던 반면, Minsky는 해답이 방치된 상식 추론과 같이 매우 중심적인 문제에 있다고 믿었다. 반면에 John McCarthy는 여전히 자격문제를 비난하였다.[138] Ray Kurzweil은 문제는 컴퓨터 성능에 있으며 무어의 법칙을 사용하였을 때 인간 수준의 지능을 가진 기계는 약 2029년에 나올 것이라고 예견하였다.[139] Jeff Hawkins는 신경망 연구자들이 대뇌 피질의 본질적인 성질을 무시하고 간단한 문제들을 성공적으로 해결하는 간단한 모델을 추구했다고 말했다.[140] 또한 각각에 대해 많은 설명들이 있으며 이를 대응하는 진행 중인 연구 프로그램들이 있다.

인공지능과 4차 산업혁명
세계는 이미 4차 산업혁명에 진입했으며 인공지능은 빠르게 인간을 대체해 나갈 것이다. 또, 널리 퍼져 있지 않을 뿐 미래는 이미 와 있으며 인공지능, IoT, 클라우드 컴퓨팅, 빅데이터 등이 융합되면서 4차 산업혁명이 발생하고 있다. 과거 산업혁명이 ‘기계근육’을 만드는 과정이었다면 4차 혁명에서는 ‘기계두뇌’가 탄생할 것이다.[141]

제1차 산업혁명 발생시, 산업 기계에 의해 일자리를 잃을 것이 두려웠던 노동자들이 러다이트(기계파괴운동)를 일으켰다. 이와 유사하게, 인공 지능에 의한 4차 산업혁명으로, 많은 사람들이 미래에 일자리를 잃을 것을 우려하고 있다. 한 온라인 설문조사[142]에 따르면, 응답자의 70.1%가 미래에 인공지능에 의해 인간의 직업이 줄어들 것이라고 예상했다.

실험적인 AI 연구
인공지능은 1959년에 MIT AI연구소를 설립한 매카시와 마빈 민스키, 카네기멜론 대학교에 인공지능 연구소를 만든 앨런 뉴웰과 허버트 사이먼과 같은 개척자들에 의해 1950년도에 실험 학문으로 시작되었다. 이들 모두는 1956년에 매카시, 민스키, IBM의 나단 로체스터 와 클라우드 샤논에 의해 조직되어 열린, 이미 언급된 다트머스 대학의 여름 AI 콘퍼런스에 참가하였다.

역사적으로, 인공지능 연구는 두 개의 부류 -- 깔끔이(Neats)와 지저분이(Scruffies) -- 로 나눌 수 있다. 깔끔이는 우리가 전통적 혹은 기호적(symbolic) 인공지능 연구라고 부르는 분야로서, 일반적으로 추상적인 개념에 대한 기호적 연산과 전문가 시스템(expert systems)에 사용된 방법론을 가르친다. 이와 상반된 영역을 우리는 지저분이(Scruffies) 또는 연결주의자(connectionist)라 부르는데, 시스템을 구축하여 지능을 구현/진화시키려고 시도하고, 특정 임무를 완수하기 위해 규칙적인 디자인을 하기보다는 자동화된 프로세스에 의해서 지능을 향상시키는 방식이다. 가장 대표적인 예로 신경망(neural network)이 있다. 이 두 가지 접근법은 인공지능 역사의 매우 초창기부터 함께 했다. 1960년대와 1970년대를 거치며 scruffy 접근법은 주목받지 못했지만, 1980년대 깔끔이 접근법의 한계가 명확해지면서 다시 주목 받게 되었다. 그러나 현재 두 가지 방식을 사용하는 그 어떤 최신의 인공지능 알고리즘도 확실한 한계가 있다는 것이 명확하다.

특히 1980년대에 들어서 Back propagation (인공지능 학습방법: Training Method)가 소개되면서 많은 연구가 진행되었음에도, 신경망을 이용한 인공지능은 아직 초보단계이다. 인공신경망 (Artificial Neural Networks)을 이용한 많은 연구가 현재에도 진행되고 있지만, 몇 가지 장애로 인해서 실용화하기엔 아직도 먼 기술이다. 인공신경망을 이용한 인공지능이 어느 정도 실용화되기 위해선 우선 실효성 있는 학습방법 (Training Methods)이 필요하다. Back propagation을 이용한 학습방법이 제안되어 연구되고 있지만, 완전한 학습을 이룰 수 없을 뿐만 아니라, 학습에 사용되는 data들이 서로 orthonormal해야 하는 조건 때문에 항상 불완전한 학습으로 끝나기 쉽다. (Converge to Local Mimimum, not to the optimal minimum: 지역최적해에 머뭄. 즉, 눈먼 장님이 가장 낮은 저지대를 찾는 경우 각 현재 지점에서 아래로 내려가려는 성질이 있는데 이때 눈먼 봉사이므로 특정 지점의 저지대에 도달한 경우, 그 지점에선 어디로 가거나 위로 올라가는 것만 있으므로 앞에 설명한 성질에 의해 바로 전에 찾은 저지대 남으려 하는 성질이 있다는 것을 의미함). 이러한 단점들을 보완하기 위해서 Fuzzy Logic, Neurofuzzy (Neural fuzzy logic) and Genetic Algorithms등을 이용한 학습방법이 연구되고 있으나 전망이 밝지만은 않은 상태이다.

미국의 DARPA(미 국방부 최신 기술 연구 프로젝트 관리국)과 일본의 5세대 컴퓨터 프로젝트에 의해서 1980년대 인공지능 연구는 엄청난 연구 기금을 지원 받을 수 있었다. 몇몇 인공지능 선각자들이 거둔 주목할 만한 결과에도 불구하고, 즉각적인 결과를 산출하는 데 실패하게 된다. 이것은 1980년대 후반 인공지능 연구 기금에 대한 대폭적인 삭감을 초래하였고, 인공지능 연구의 침체기를 뜻하는 인공지능의 겨울을 가져왔다